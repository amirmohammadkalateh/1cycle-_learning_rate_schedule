# 1cycle_learning_rate_schedule
```markdown
# Simple ANN with 1Cycle Learning Rate Schedule (TensorFlow/Keras)

This repository contains a simple Artificial Neural Network (ANN) model built using the TensorFlow Keras Sequential API. It demonstrates the implementation and visualization of the 1Cycle learning rate schedule using the `LearningRateScheduler` callback.

## Overview

The 1Cycle learning rate policy, popularized by Leslie Smith, is a technique that aims to improve training speed and generalization by varying the learning rate cyclically. It typically involves two phases:

1.  **Warm-up Phase:** The learning rate increases linearly from a small initial value to a maximum value.
2.  **Cool-down Phase:** The learning rate decreases linearly (or sometimes more aggressively) from the maximum value to a much smaller value.

This repository provides:

* A basic sequential ANN model with one hidden layer.
* A function to create a 1Cycle learning rate schedule.
* Implementation of the schedule using the `LearningRateScheduler` callback.
* Visualization of the learning rate changes during training.
* (Optional) Visualization of the training loss and accuracy.

## Getting Started

### Prerequisites

* Python 3.x
* TensorFlow (>= 2.x)
* NumPy
* Matplotlib

You can install the necessary libraries using pip:

```bash
pip install tensorflow numpy matplotlib
```

### Usage

1.  Clone this repository:

    ```bash
    git clone <repository_url>
    cd simple-ann-1cycle-lr
    ```

2.  Run the Python script:

    ```bash
    python main.py
    ```

    This script will:
    * Define and compile the simple ANN model.
    * Generate dummy training data.
    * Create and apply the 1Cycle learning rate schedule using a callback.
    * Train the model.
    * Display plots visualizing the learning rate schedule and (optionally) the training loss and accuracy.

## Code Structure

* `main.py`: The main Python script containing the ANN model definition, 1Cycle learning rate schedule implementation, training process, and visualization code.

## Understanding the 1Cycle Learning Rate Schedule

The `create_one_cycle_lr_schedule` function in `main.py` defines the logic for the learning rate variation over the training steps. It divides the training into two equal phases: increasing the learning rate and then decreasing it.

The visualization generated by the script will clearly show this cyclical behavior of the learning rate.

## Further Exploration

* **Experiment with different hyperparameters:** Modify the number of layers, neurons, activation functions, maximum learning rate, and the shape of the learning rate cycle.
* **Implement momentum scheduling:** For a more complete 1Cycle policy, you could explore implementing a custom callback to adjust the optimizer's momentum along with the learning rate (though this is more advanced with optimizers like Adam).
* **Apply to real datasets:** Replace the dummy data with a real-world dataset to observe the impact of the 1Cycle learning rate schedule on a specific task.
* **Compare with other learning rate schedules:** Experiment with standard learning rate decay schedules (e.g., exponential decay, step decay) and compare their performance with the 1Cycle policy.

